---
author: "Blake Miller"
output:
  html_document: default
  pdf_document: default
---

# Supervised learning applied to text

## Naive Bayes

The code here illustrates how we can use supervised machine learning to predict categories for unseen documents based on a set of labeled documents. Our running example will focus on whether we can predict gender based on the character distribution of first names.

The file `data/EN-names.csv` contains a list of nearly 25,000 popular names in the US labeled by the most frequent gender based on Social Security records.

Let's read this dataset into R, convert it into a corpus with gender as a document-level variable.

```{r}
library("quanteda", quietly = TRUE, warn.conflicts = FALSE, verbose = FALSE)

d <- read.csv("data/EN-names.csv", stringsAsFactors=FALSE)

# creating corpus object
cnames <- corpus(d, text_field = "name")
docvars(cnames, "gender") <- d$gender
```

As we saw in the lecture, we need to specify what the training set and test set will be. In this case, let's just take an 80% random sample of names as training set and the rest as test set, which we will use to compute the performance of our model. We will then create a document-feature matrix where each feature is a character.

```{r}
# shuffling to split into training and test set
smpl <- sample(c("train", "test"), size=ndoc(cnames), 
                prob=c(0.80, 0.20), replace=TRUE)
table(smpl)
train <- which(smp=="train")
test <- which(smp=="test")

# tokenizing and creating DFM
characters <- tokens(cnames, what="character")
namesdfm <- dfm(characters)
```

We're now ready to train our model! Let's start with a Naive Bayes model using the `textmodel_nb()` function:

```{r}
# training Naive Bayes model
nb <- textmodel_nb(namesdfm[train,], docvars(cnames, "gender")[train])
# predicting labels for test set
preds <- predict(nb, newdata = namesdfm[test,])
# computing the confusion matrix
(cm <- table(preds, docvars(cnames, "gender")[test]))
```

How well did we do? We can compute precision, recall, and accuracy to quantify it.

```{r}
# function to compute performance metrics
precrecall <- function(mytable, verbose=TRUE) {
    truePositives <- mytable[1,1]
    falsePositives <- sum(mytable[1,]) - truePositives
    falseNegatives <- sum(mytable[,1]) - truePositives
    precision <- truePositives / (truePositives + falsePositives)
    recall <- truePositives / (truePositives + falseNegatives)
    if (verbose) {
        print(mytable)
        cat("\n precision =", round(precision, 2), 
            "\n    recall =", round(recall, 2), "\n")
    }
    invisible(c(precision, recall))
}

# precision and recall
precrecall(cm)
# accuracy
sum(diag(cm)) / sum(cm)
```

Hmm, not terribly great. But what if we try with character n-grams up to trigrams instead of unigrams?

```{r}
characters <- tokens(cnames, what="character", ngrams=1:3)
namesdfm <- dfm(characters)
# Naive Bayes model
nb <- textmodel_nb(namesdfm[train,], docvars(cnames, "gender")[train])
preds <- predict(nb, newdata = namesdfm[test,])
(cm <- table(preds, docvars(cnames, "gender")[test]))
# performance
precrecall(cm) # precision, recall
sum(diag(cm)) / sum(cm) # accuracy
```

Slightly better! We can dig a bit more into the model by extracting the posterior class probabilities for specific characters.

```{r}
# extracting posterior word probabilities
probs <- nb$PcGw
probs[,c("a", "o", "e")]

# and this is how we would extract the features with the highest and lowest posterior class probabilities
df <- data.frame(
  ngram = colnames(probs),
  prob_female_ngram = probs["female",],
  stringsAsFactors=F)
df <- df[order(df$prob_female_ngram),]

head(df, n=10)
tail(df, n=10)
```


## Regularized regression

We'll now switch to the other type of classifier we just saw in the lecture - a regularized regression. This model is not implemented in quanteda, but we can use one of the other available packages in R. For regularized regression, glmnet is in my opinion the best option, since it tends to be faster than caret or mlr (in my experience at least), and it has cross-validation already built-in, so we don’t need to code it from scratch. 

We’ll start with a ridge regression:

```{r}
# install.packages("glmnet")
library(glmnet)

ridge <- cv.glmnet(x=namesdfm[train,], y=docvars(cnames, "gender")[train],
                   alpha=0, nfolds=5, family="binomial")
```

We use the `cv.glmnet()` function, with the following options: `alpha` indicates whether we want a ridge penalty (`alpha=0`) or a lasso penalty (`alpha=1`), `nfolds` is the number of K folds for the cross-validation procedure, and `family` indicates the type of classifier (`binomial` means binary here).

It's generally good practice to plot the results of the cross-validation procedure.

```{r}
plot(ridge)
```

What do we learn from this plot? It shows the error (with confidence intervals based on the cross-validation procedure) for each possible value of lambda (the penalty parameter). The numbers on top indicate the number of features (which remain constant with ridge, unlike with lasso). We generally find that increasing the penalty parameter actually hurts.

Let's now compute different performance metrics to see how we're doing now.

```{r}
pred <- predict(ridge, namesdfm[test,], type="class")
(cm <- table(pred, docvars(cnames, "gender")[test]))

# performance metrics
precrecall(cm) # precision, recall
sum(diag(cm)) / sum(cm) # accuracy
```

Not bad! And with a regularized regression, in a similar way as we did earlier with the Naive Bayes model, we can also extract the feature-specific coefficients to try to understand how the latent dimension we're capturing here can be interpret.

```{r}
# extracting coefficients
best.lambda <- which(ridge$lambda==ridge$lambda.1se)
beta <- ridge$glmnet.fit$beta[,best.lambda]

## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
                ngram = names(beta), stringsAsFactors=F)

# lowest and highest coefficients
df <- df[order(df$coef),]
head(df[,c("coef", "ngram")], n=10)
tail(df[,c("coef", "ngram")], n=10)
```

The code below shows how to re-run the analysis but this time with lasso. Note that this time the number of features will change depending on the value of the penalty parameter.

```{r}
# now with lasso
lasso <- cv.glmnet(x=namesdfm[train,], y=docvars(cnames, "gender")[train],
                   alpha=1, nfolds=5, family="binomial")
plot(lasso)

pred <- predict(lasso, namesdfm[test,], type="class")
(cm <- table(pred, docvars(cnames, "gender")[test]))

# precision and recall
precrecall(cm)
# accuracy
sum(diag(cm)) / sum(cm)

# extracting coefficients
best.lambda <- which(lasso$lambda==lasso$lambda.1se)
beta <- lasso$glmnet.fit$beta[,best.lambda]

## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
                ngram = names(beta), stringsAsFactors=F)

# note that some features become 0
table(df$coef==0)

df <- df[order(df$coef),]
head(df[,c("coef", "ngram")], n=10)
tail(df[,c("coef", "ngram")], n=10)
```