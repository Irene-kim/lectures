---
title: "Dictionary methods"
author: Blake Miller
output: html_document
---

## Sentiment detection using dictionary methods

One of the most common applications of dictionary methods is sentiment analysis: using a dictionary of positive and negative words, we compute a sentiment score for each individual document.

Let's apply this technique to tweets by the four leading candidates in the 2016 Presidential primaries. Which candidate was using positive rhetoric most frequently? Which candidate was most negative in their public messages on Twitter?

```{r}
library(quanteda)
tweets <- read.csv('candidate-tweets.csv', stringsAsFactors=F)
```

We will use the positive and negative categories in the augmented General Inquirer dictionary to measure the extent to which these candidates adopted a positive or negative tone during the election campaign.

Note that first you will need to install the `quanteda.dictionaries` package from GitHub

```{r, eval=FALSE}
devtools::install_github("kbenoit/quanteda.dictionaries")
```

First, we load the dictionary object. Note that we can apply the dictionary object directly (as we will see later), but for now let's learn how to do this if we had a list of positive and negative words on a different file.

```{r}
library(quanteda.dictionaries)
data(data_dictionary_geninqposneg)

pos.words <- data_dictionary_geninqposneg[['positive']]
neg.words <- data_dictionary_geninqposneg[['negative']]
# a look at a random sample of positive and negative words
sample(pos.words, 10)
sample(neg.words, 10)
```

As earlier in the course, we will convert our text to a corpus object. Note that here `corpus` takes detects some metadata, which we will use later.

```{r}
twcorpus <- corpus(tweets)
```

Now we're ready to run the sentiment analysis! First we will construct a dictionary object.

```{r}
mydict <- dictionary(list(positive = pos.words,
                          negative = neg.words))
```

And now we apply it to the corpus in order to count the number of words that appear in each category

```{r}
sent <- dfm(twcorpus, dictionary = mydict)
```

We can then extract the score and add it to the data frame as a new variable

```{r}
tweets$score <- as.numeric(sent[,1]) - as.numeric(sent[,2])
head(tweets$score)
```

And now start answering some descriptive questions...

```{r}
# what is the average sentiment score?
mean(tweets$score)
# what is the most positive and most negative tweet?
tweets[which.max(tweets$score),]
tweets[which.min(tweets$score),]
# what is the proportion of positive, neutral, and negative tweets?
# overwrite the values 
tweets$sentiment <- "neutral"
tweets$sentiment[tweets$score<0] <- "negative"
tweets$sentiment[tweets$score>0] <- "positive"
table(tweets$sentiment)
```

We can also compute it at the candidate level by taking the average of the sentiment scores:

```{r}
# average sentiment of specific candidates
# loop over candidates
candidates <- c("realDonaldTrump", "HillaryClinton", "tedcruz", "BernieSanders")

for (cand in candidates){
  message(cand, " -- average sentiment: ",
      round(mean(tweets$score[tweets$screen_name==cand]), 4)
    )
}

# a single word might be driving the entire result. Thus, we need to validate.
```

But what happens if we now run the analysis excluding a single word?
The result shows how a single word "great" is driving the entire result in dictionary method. 

```{r}
pos.words <- pos.words[-which(pos.words=="great")]

mydict <- dictionary(list(positive = pos.words,
                          negative = neg.words))
sent <- dfm(twcorpus, dictionary = mydict)
tweets$score <- as.numeric(sent[,1]) - as.numeric(sent[,2])

for (cand in candidates){
  message(cand, " -- average sentiment: ",
      round(mean(tweets$score[tweets$screen_name==cand]), 4)
    )
}

```

How would we normalize by text length? (Maybe not necessary here given that tweets have roughly the same length.)
Just because your text is longer - which means it has more words might mislead the results. 

```{r}
# collapse by account into 4 documents
twdfm <- dfm(twcorpus, groups = "screen_name")
twdfm

# turn word counts into proportions
twdfm[1:4, 1:10]
twdfm <- dfm_weight(twdfm, scheme="prop")
twdfm[1:4, 1:10]

# Apply dictionary using `dfm_lookup()` function:
sent <- dfm_lookup(twdfm, dictionary = mydict)
sent
# remove negative and mulitply 100 to the postive feature
(sent[,1]-sent[,2])*100

```

Finally, let's apply a different dictionary so that we can practice with dictionaries in different formats:

```{r}
data(data_dictionary_MFD)
# dictionary keys
names(data_dictionary_MFD)
# looking at words within first key
data_dictionary_MFD$care.virtue[1:10]

# applying dictionary
# 1) collapse by account
twdfm <- dfm(twcorpus, groups = "screen_name")
# 2) turn words into proportions
twdfm <- dfm_weight(twdfm, scheme="prop")
# 3) apply dictionary
moral <- dfm_lookup(twdfm, dictionary = data_dictionary_MFD)
moral

# are liberals more sensitive to care and virtue? Are Hiliary and Bernie gonna use more care-virtue than other candidates? Do they talk about fairness more than other candidates?
sort(moral[,'care.virtue']*100)
sort(moral[,'fairness.virtue']*100)

# are conservatives more sensitive to sanctity and authority?
# the more authoritarian the parenting style they are the more likely they would support Trump.
sort(moral[,'sanctity.virtue']*100)
sort(moral[,'authority.virtue']*100)

```

## Identifying most unique features of documents

_Keyness_ is a measure of to what extent some features are specific to a (group of) document in comparison to the rest of the corpus, taking into account that some features may be too rare.

```{r}
#group by politions 
twdfm <- dfm(twcorpus, groups=c("screen_name"), verbose=TRUE)

# Donald Trump
# extstat_keyness - do chi-square test and do independence test 
#textstat_keyness: Nice way to see discriptive statistics

head(textstat_keyness(twdfm, target="realDonaldTrump",
                      measure="chi2"), n=20)
textstat_keyness(twdfm, target="realDonaldTrump",
                      measure="chi2") %>% textplot_keyness()

# Hillary Clinton
head(textstat_keyness(twdfm, target="HillaryClinton",
                      measure="chi2"), n=20)
textstat_keyness(twdfm, target="HillaryClinton",
                      measure="chi2") %>% textplot_keyness()

# Ted Cruz
head(textstat_keyness(twdfm, target="tedcruz",
                      measure="chi2"), n=20)
textstat_keyness(twdfm, target="tedcruz",
                      measure="chi2") %>% textplot_keyness()

# Bernie Sanders
head(textstat_keyness(twdfm, target="BernieSanders",
                      measure="chi2"), n=20)
textstat_keyness(twdfm, target="BernieSanders",
                      measure="chi2") %>% textplot_keyness()

```

If we have other metadata in our corpus, we can also use it to identify words whose usage varies over time:

```{r}
trump <- corpus_subset(twcorpus, screen_name=="realDonaldTrump")
twdfm <- dfm(trump, remove_punct=TRUE,
             remove=c(stopwords("english"), 'rt', 'u', 's'), verbose=TRUE)

textstat_keyness(twdfm, target=docvars(twdfm)$datetime>"2016-05", 
                      measure="chi2") %>% textplot_keyness()

```

#classifier approach - deep learning ->there are lots of available word vectors depending on what type of sentiment you want to meausre. If you have strong domain knowledge you might just want to use dictionary method rather than spending alot of time labelling 